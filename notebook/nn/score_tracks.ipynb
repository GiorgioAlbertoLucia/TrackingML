{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tracks( truth, submission):\n",
    "        \"\"\"Compute the majority particle, hit counts, and weight for each track.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        truth : pandas.DataFrame\n",
    "            Truth information. Must have hit_id, particle_id, and weight columns.\n",
    "        submission : pandas.DataFrame\n",
    "            Proposed hit/track association. Must have hit_id and track_id columns.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pandas.DataFrame\n",
    "            Contains track_id, nhits, major_particle_id, major_particle_nhits,\n",
    "            major_nhits, and major_weight columns.\n",
    "        \"\"\"\n",
    "\n",
    "        # true number of hits for each particle_id\n",
    "        particles_nhits = truth['particle_id'].value_counts(sort=False)\n",
    "        total_weight = truth['weight'].sum()\n",
    "        # combined event with minimal reconstructed and truth information\n",
    "        event = pd.merge(truth[['hit_id', 'particle_id', 'weight']],\n",
    "                             submission[['hit_id', 'track_id']],\n",
    "                             on=['hit_id'], how='left', validate='one_to_one')\n",
    "        event.drop('hit_id', axis=1, inplace=True)\n",
    "        event.sort_values(by=['track_id', 'particle_id'], inplace=True)\n",
    "\n",
    "        # ASSUMPTIONs: 0 <= track_id, 0 <= particle_id\n",
    "\n",
    "        tracks = []\n",
    "        # running sum for the reconstructed track we are currently in\n",
    "        rec_track_id = -1\n",
    "        rec_nhits = 0\n",
    "        # running sum for the particle we are currently in (in this track_id)\n",
    "        cur_particle_id = -1\n",
    "        cur_nhits = 0\n",
    "        cur_weight = 0\n",
    "        # majority particle with most hits up to now (in this track_id)\n",
    "        maj_particle_id = -1\n",
    "        maj_nhits = 0\n",
    "        maj_weight = 0\n",
    "\n",
    "        for hit in tqdm(event.itertuples(index=False)):\n",
    "            # we reached the next track so we need to finish the current one\n",
    "            if (rec_track_id != -1) and (rec_track_id != hit.track_id):\n",
    "                # could be that the current particle is the majority one\n",
    "                if maj_nhits < cur_nhits:\n",
    "                    maj_particle_id = cur_particle_id\n",
    "                    maj_nhits = cur_nhits\n",
    "                    maj_weight = cur_weight\n",
    "                # store values for this track\n",
    "                tracks.append((rec_track_id, rec_nhits, maj_particle_id,\n",
    "                    particles_nhits[maj_particle_id], maj_nhits,\n",
    "                    maj_weight / total_weight))\n",
    "\n",
    "            # setup running values for next track (or first)\n",
    "            if rec_track_id != hit.track_id:\n",
    "                rec_track_id = hit.track_id\n",
    "                rec_nhits = 1\n",
    "                cur_particle_id = hit.particle_id\n",
    "                cur_nhits = 1\n",
    "                cur_weight = hit.weight\n",
    "                maj_particle_id = -1\n",
    "                maj_nhits = 0\n",
    "                maj_weights = 0\n",
    "                continue\n",
    "\n",
    "            # hit is part of the current reconstructed track\n",
    "            rec_nhits += 1\n",
    "\n",
    "            # reached new particle within the same reconstructed track\n",
    "            if cur_particle_id != hit.particle_id:\n",
    "                # check if last particle has more hits than the majority one\n",
    "                # if yes, set the last particle as the new majority particle\n",
    "                if maj_nhits < cur_nhits:\n",
    "                    maj_particle_id = cur_particle_id\n",
    "                    maj_nhits = cur_nhits\n",
    "                    maj_weight = cur_weight\n",
    "                # reset runnig values for current particle\n",
    "                cur_particle_id = hit.particle_id\n",
    "                cur_nhits = 1\n",
    "                cur_weight = hit.weight\n",
    "            # hit belongs to the same particle within the same reconstructed track\n",
    "            else:\n",
    "                cur_nhits += 1\n",
    "                cur_weight += hit.weight\n",
    "\n",
    "        # last track is not handled inside the loop\n",
    "        if maj_nhits < cur_nhits:\n",
    "            maj_particle_id = cur_particle_id\n",
    "            maj_nhits = cur_nhits\n",
    "            maj_weight = cur_weight\n",
    "        # store values for the last track\n",
    "        tracks.append((rec_track_id, rec_nhits, maj_particle_id,\n",
    "            particles_nhits[maj_particle_id], maj_nhits, maj_weight / total_weight))\n",
    "\n",
    "        cols = ['track_id', 'nhits',\n",
    "                'major_particle_id', 'major_particle_nhits',\n",
    "                'major_nhits', 'major_weight']\n",
    "        return pd.DataFrame.from_records(tracks, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f21aaa9573847669300fcc02ed70be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('/Users/glucia/Projects/DeepLearning/TrackingML/data/save/test_reco_tracks.csv')\n",
    "\n",
    "truth = dataframe[['hit_id', 'particle_id', 'weight']].copy()\n",
    "submission = dataframe[['hit_id', 'track_id']].copy()\n",
    "\n",
    "analized_tracks = analyze_tracks(truth, submission)\n",
    "purity_rec = np.divide(analized_tracks['major_nhits'], analized_tracks['nhits'])\n",
    "purity_maj = np.divide(analized_tracks['major_nhits'], analized_tracks['major_particle_nhits'])\n",
    "good_track_mask = (purity_rec > 0.5) & (purity_maj > 0.5)\n",
    "#score = analized_tracks[good_track_mask]['major_weight'].sum()\n",
    "score = analized_tracks['major_weight'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Score: 0.1532\n",
      "\t* Purity rec: 0.7052\n",
      "\t* Purity maj: 0.6627\n"
     ]
    }
   ],
   "source": [
    "print(f'\\t* Score: {score:.4f}')\n",
    "print(f'\\t* Purity rec: {np.mean(purity_rec[good_track_mask]):.4f}')\n",
    "print(f'\\t* Purity maj: {np.mean(purity_maj[good_track_mask]):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyROOT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
