{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First attempt using a Fully Connected Neural Network (FCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "from alive_progress import alive_bar\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/glucia/Projects/DeepLearning/TrackingML')\n",
    "from core.data_handler import DataHandler\n",
    "from core.nn import FCNN\n",
    "from core.losses import ContrastiveLoss\n",
    "from utils.terminal_colors import TerminalColors as tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mNumber of events: 1770\u001b[0m\n",
      "\u001b[32mNumber of events used: \u001b[0m\u001b[1m2\u001b[0m\n",
      "DataHandler object.\n",
      "Dataset:\n",
      "\t- type: <class 'pandas.core.frame.DataFrame'>\n",
      "\t- length: 330825\n",
      "\t- columns: (['x', 'y', 'z', 'value', 'event_id'], ['particle_id'])\n",
      "x: \n",
      "\t- shape: torch.Size([330825, 4])\n",
      "\t- type: <class 'torch.Tensor'>\n",
      "y: \n",
      "\t- shape: torch.Size([330825])\n",
      "\t- type: <class 'torch.Tensor'>\n",
      "\n",
      "Batch 0\n",
      "  data: torch.Size([330825, 4])\n",
      "  target: torch.Size([330825])\n"
     ]
    }
   ],
   "source": [
    "# Load data (only 2 events)\n",
    "\n",
    "input_dir = '../../data/train_1'\n",
    "event_numbers = []\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\"-cells.csv\"):\n",
    "        event_number = filename.split(\"-\")[0][5:]\n",
    "        event_numbers.append(event_number)\n",
    "event_numbers.sort()\n",
    "print(tc.BOLD+f'Number of events: {len(event_numbers)}'+tc.RESET)\n",
    "\n",
    "cell_csvs = [input_dir+f'/event{event_number}-cells.csv' for event_number in event_numbers]\n",
    "hits_csvs = [input_dir+f'/event{event_number}-hits.csv' for event_number in event_numbers]\n",
    "truth_csvs = [input_dir+f'/event{event_number}-truth.csv' for event_number in event_numbers]\n",
    "\n",
    "EVENTS_USED = 2\n",
    "print(tc.GREEN+'Number of events used: '+tc.RESET+tc.BOLD+f'{EVENTS_USED}'+tc.RESET)\n",
    "\n",
    "data_handler = DataHandler(event_numbers[:2], cell_csvs[:2], hits_csvs[:2], truth_csvs[:2])\n",
    "\n",
    "N_PARTICLES = 100\n",
    "train_set = copy.deepcopy(data_handler)\n",
    "train_set.dataset.query('event_id==1000', inplace=True)\n",
    "train_set.reduce_to_n_particles(N_PARTICLES)\n",
    "\n",
    "test_set = copy.deepcopy(data_handler)\n",
    "test_set.dataset.query('event_id==1001', inplace=True)\n",
    "\n",
    "mean = train_set.mean()\n",
    "std = train_set.std()\n",
    "\n",
    "train_set.feature_scaling(mean, std)\n",
    "test_set.feature_scaling(mean, std)\n",
    "\n",
    "print(train_set)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=len(train_set), shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=len(test_set), shuffle=True)\n",
    "\n",
    "# get train_loader informations\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    print(f'Batch {batch_idx}')\n",
    "    print(f'  data: {data.shape}')\n",
    "    print(f'  target: {target.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gpu if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = FCNN(input_dimension=4, output_dimension=3)\n",
    "criterion = ContrastiveLoss(margin=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track pairs of hits\n",
    "def track_pairs(data_loader:DataLoader):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    for batch_idx, (hits, truth) in enumerate(data_loader):\n",
    "        for ihit1 in range(len(hits)):\n",
    "            for ihit2 in range(ihit1+1, len(hits)):\n",
    "                pairs.append([hits[ihit1], hits[ihit2]])\n",
    "                labels.append(truth[ihit1] == truth[ihit2])\n",
    "    return pairs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on 0: Epoch 1\n",
      "|▍⚠︎                                      | (!) 1/100 [1%] in 11:44.4 (0.00/s) \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     16\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 17\u001b[0m train_pairs, train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mtrack_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (input1, input2), label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(train_pairs, train_labels):\n\u001b[1;32m     20\u001b[0m     input1, input2, label \u001b[38;5;241m=\u001b[39m input1\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), input2\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), torch\u001b[38;5;241m.\u001b[39mtensor([label], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m, in \u001b[0;36mtrack_pairs\u001b[0;34m(data_loader)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m ihit2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ihit1\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(hits)):\n\u001b[1;32m      8\u001b[0m             pairs\u001b[38;5;241m.\u001b[39mappend([hits[ihit1], hits[ihit2]])\n\u001b[0;32m----> 9\u001b[0m             labels\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtruth\u001b[49m\u001b[43m[\u001b[49m\u001b[43mihit1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtruth\u001b[49m\u001b[43m[\u001b[49m\u001b[43mihit2\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pairs, labels\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop with early stopping\n",
    "NUM_EPOCHS = 100\n",
    "PATIENCE = 10\n",
    "BEST_VAL_LOSS = float('inf')\n",
    "PATIENCE_COUNTER = 0\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "with alive_bar(NUM_EPOCHS) as bar:\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        if epoch%10 == 0:\n",
    "            print(f'Epoch {epoch+1}')\n",
    "        bar()\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_pairs, train_labels = track_pairs(train_loader)\n",
    "\n",
    "        for (input1, input2), label in zip(train_pairs, train_labels):\n",
    "            input1, input2, label = input1.unsqueeze(0), input2.unsqueeze(0), torch.tensor([label], dtype=torch.float32)\n",
    "            optimizer.zero_grad()\n",
    "            output1 = model(input1)\n",
    "            output2 = model(input2)\n",
    "            loss = criterion(output1, output2, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_pairs)\n",
    "\n",
    "        train_losses.append(train_loss.detach())\n",
    "\n",
    "        # Validation\n",
    "        #model.eval()\n",
    "        #val_loss = 0.0\n",
    "        #val_pairs, val_labels = track_pairs(train_loader)\n",
    "        #with torch.no_grad():\n",
    "        #    for (input1, input2), label in zip(val_pairs, val_labels):\n",
    "        #        input1, input2, label = input1.unsqueeze(0), input2.unsqueeze(0), torch.tensor([label], dtype=torch.float32)\n",
    "        #        output1 = model(input1)\n",
    "        #        output2 = model(input2)\n",
    "        #        loss = criterion(output1, output2, label)\n",
    "        #        val_loss += loss.item()\n",
    "    #\n",
    "        #val_loss /= len(val_pairs)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {train_loss:.4f}') # , Val Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Early stopping\n",
    "        #if val_loss < best_val_loss:\n",
    "        #    best_val_loss = val_loss\n",
    "        #    patience_counter = 0\n",
    "        #    best_model_weights = model.state_dict()\n",
    "        #else:\n",
    "        #    patience_counter += 1\n",
    "        #    if patience_counter >= PATIENCE:\n",
    "        #        print(\"Early stopping\")\n",
    "        #        break\n",
    "\n",
    "    # Load best model weights\n",
    "    #model.load_state_dict(best_model_weights)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(train_losses, label = \"Train\")\n",
    "#plt.plot(train_accuracies, label = \"Train\")\n",
    "#plt.plot(test_accuracies, label = \"Test\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#print(train_accuracies[-1])\n",
    "#print(test_accuracies[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyROOT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
